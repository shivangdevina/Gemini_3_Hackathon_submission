# -*- coding: utf-8 -*-
"""research_work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHsOigfoU0YdNtIsjKjV0PWRC3grv2cR
"""


import time
from typing import TypedDict, List,Dict
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv, find_dotenv

import os
_ = load_dotenv(find_dotenv())
api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    google_api_key=api_key
)

from typing import List, Dict, TypedDict
import json
import re

from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI

class Role(TypedDict):
    title: str
    weight: float

class Person(TypedDict):
    name: str
    role: Dict[str, float]
    skills: List[str]

class ResearchAssignment(TypedDict):
    topic: str
    assigned_to: str
    justification: str

class ProjectState(TypedDict):
    problem_statement: str
    team: List[Person]
    raw_topics: List[str]
    refined_topics: List[str]
    assignments: List[ResearchAssignment]

def safe_llm_invoke(prompt, retries=3, wait=25):
    for attempt in range(retries):
        try:
            return llm.invoke(prompt)
        except Exception as e:
            if "RESOURCE_EXHAUSTED" in str(e):
                print(f"[WARN] Quota hit. Retrying in {wait}s...")
                time.sleep(wait)
            else:
                raise e
    raise RuntimeError("LLM quota exhausted")

def extract_json(text: str):
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if not match:
        raise ValueError("No JSON array found in LLM output")
    return json.loads(match.group())

def generate_research_topics(state: ProjectState) -> ProjectState:
    num_topics = len(state["team"]) # Ensure we have enough tasks for everyone

    prompt = f"""
You are a senior hackathon mentor.
Problem Statement: {state["problem_statement"]}

Generate exactly {num_topics} distinct research topics.
- Ensure the topics cover a broad range (e.g., ML, Frontend/UX, Backend, Research/Domain, and Deployment).
- This ensures that every specialist on the team has a task relevant to their field.

Return ONLY a bullet list of {num_topics} topics.
"""
    response = llm.invoke(prompt)
    topics = [
        line.strip("- ").strip()
        for line in response.content.split("\n")
        if line.strip()
    ][:num_topics] # Safety slice

    state["raw_topics"] = topics
    return state

def refine_topics(state: ProjectState) -> ProjectState:
    prompt = f"""
Refine these research topics to be actionable in 2â€“4 hours:

{state["raw_topics"]}

Return ONLY a bullet list.
"""

    response = llm.invoke(prompt)

    refined = [
        line.strip("- ").strip()
        for line in response.content.split("\n")
        if line.strip()
    ]

    state["refined_topics"] = refined
    return state

def assign_topics_with_llm(state: ProjectState) -> ProjectState:
    # Calculate team size to ensure 1:1 mapping
    num_members = len(state["team"])

    prompt = f"""
You are a senior technical lead.

We have a team of {num_members} people. You MUST assign exactly ONE research topic to EACH team member.

Rules:
1. Every single person in the team list must have an assignment.
2. Use their 'role' weights and 'skills' list to give them the MOST relevant topic for their expertise.
3. If a topic doesn't perfectly match a role, assign it to the person whose general skills are the best fit.
4. Output ONLY a JSON array of objects with keys: "topic", "assigned_to", and "justification".

Team:
{state["team"]}

Topics:
{state["refined_topics"]}

Return JSON ONLY.
"""
    response = llm.invoke(prompt)
    state["assignments"] = extract_json(response.content)
    return state

graph = StateGraph(ProjectState)

graph.add_node("generate_topics", generate_research_topics)
graph.add_node("refine_topics", refine_topics)
graph.add_node("assign_topics", assign_topics_with_llm)

graph.set_entry_point("generate_topics")

graph.add_edge("generate_topics", "refine_topics")
graph.add_edge("refine_topics", "assign_topics")
graph.add_edge("assign_topics", END)

app = graph.compile()

team = [
        {
            "name": "Aman",
            "role": {"ML Engineer": 0.9, "Research Scientist": 0.7},
            "skills": ["python", "machine learning", "computer vision", "data preprocessing"]
        },
        {
            "name": "Riya",
            "role": {"Frontend Developer": 0.95, "UI Designer": 0.8},
            "skills": ["react", "ui/ux", "accessibility", "responsive design"]
        },
        {
            "name": "Kunal",
            "role": {"Backend Engineer": 0.85, "System Architect": 0.7},
            "skills": ["api development", "database design", "python"]
        },
        {
            "name": "Sneha",
            "role": {"Product Manager": 0.9, "Agriculture Specialist": 0.85},
            "skills": ["user research", "problem analysis", "market validation"]
        },
        {
            "name": "Arjun",
            "role": {"DevOps Engineer": 0.85, "Hardware Specialist": 0.8},
            "skills": ["cloud deployment", "edge devices", "scalability"]
        }
    ]
ps = "AI-based crop disease detection for rural farmers"

def final_call(team_members,ps):

    initial_state: ProjectState = {
    "problem_statement": ps,
    "team": team_members,
    "raw_topics": [],
    "refined_topics": [],
    "assignments": []
  }

    result = app.invoke(initial_state)
    return result

# result = final_call(team,ps)

# for a in result["assignments"]:
#     print(a)